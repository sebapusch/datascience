---
title: "Assignment 2 Data Analytics & Communication"
author: "Author: Marieke van Vugt"
output: 
  pdf_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(ROCR)
require(e1071)
require(dplyr)
```

In this assignment, you will practise with methods to assess the quality of models. While the assignment is focused on support vector machines, these same methods are usable for most other types of classifiers or algorithms as well.

The dataset used in this assignment comes from an experiment in which I examined decision making. Participants viewed displays of randomly moving dots, and had to decide whether the majority of those dots were moving in the left- or the rightward direction. They indicated their response with a button press, and we recorded their accuracy and response time. While the objective of the experiment was to examine the neural correlates of this decision making process, here we will focus on the behavioral effects. We will look at what variables in this dataset predict task performance for a single participant. The variables that we have recorded and that may potentially be relevant are:

* `cohFac`, which is a factor that represents the proportion of dots that move into a single direction (the more dots move in one direction, the easier the task, so this is essentially a measure of task difficulty). When `cohFac` is 1, the task is easier than when `cohFac` is 0 (there is also a variable called `cohVec` which are the actual proportion of dots moving in a single direction used for this participant. You will see that those values differ between participants). 
* `blocknum` (the block within the task)
* `isDots` (there are dots trials, but also a control condition in which participants don't have to really make a decision, but instead just press the button indicated by the direction of an arrow on the screen)
* `isLeft` (whether the dots are moving to the left or right)
* `ER` (whether the participant answered the trial correctly, i.e., did not make an error (0) or incorrectly, i.e., made an error (1))
* `RT` (response time in seconds)

Make sure you read the extra information about this dataset and the experiment in the file `datasetsEN.pdf`, which will help you better understand the experiment. Make sure you include your code in your answers (this is very easy when you use Rmarkdown).

# Linear SVM
We will start with making our basic regression model. In this case, we want to see whether it is possible to predict for participant 8 whether s/he is going to get a particular trial correct (so predicting the variable `ER`), you would run a logistic regression of `ER` on `RT`, `blocknum`, `isDots`, `cohFac`, and `isLeft`. Before doing so, select the data for this participant:

```{r}
# Make sure you adjust the path in read.table to align with where 
# you have stored the data file decision.dat
# This code focuses on participant number 8
dat<-read.table("/Users/mvugt/teaching/DataAnalytics&Communication/decision.dat",header=T)
subj8 <- dat[dat$subjNo==8,]
```


Then run the model using the following steps:

```{r}
# prepare training data--make sure there are no duplicate columns , and also no columns that are constant
training_data = data.frame(x = dat[,c(1,3:5,7:8)], class = as.factor(dat$ER))
training_data = data.frame(x = subj8[,c(3:5,7:8)], class = as.factor(subj8$ER))
# set apart testing data
samp <- sample(nrow(training_data),round(0.2*nrow(training_data)))
testing_data <- training_data[samp,]
training_data2 <- training_data[-samp,]
subj8training <- subj8[-samp,]

# for the training data select an equal number of each class (we need the original subj8 to find the ER as numeric so easy comparison is possible)
errorInd <- which(subj8training$ER==1)
Nerrors <- length(errorInd)
correctInd <- which(subj8training$ER==0)
sampleCorr <- sample(correctInd,Nerrors)
equal_training_data <- training_data2[c(errorInd,sampleCorr),]

# Now run the SVM
svm.linear = svm(class~., 
             data = equal_training_data, 
             kernel = "linear", 
             cost = 10, 
             scale = TRUE)
summary(svm.linear)
# do the stats on the training data
train.pred <- predict(svm.linear,equal_training_data)
# compute confusion matrix
err.tr1 <- table(equal_training_data$class,train.pred)
err.tr1
# compute error rate
tr.err1 <- (1 - (sum(diag(err.tr1))/sum(err.tr1)))
tr.err1
# now look at how the model does on the testing data
test.pred <- predict(svm.linear,testing_data)
err.te1 <- table(testing_data$class,test.pred)
err.te1
te.err1 <- (1- (sum(diag(err.te1))/sum(err.te1)))
te.err1

# for a linear SVM the feature importance can be directly extracted from the coefficients
coefficients <- t(svm.linear$coefs) %*% svm.linear$SV
importance <- abs(coefficients)
importance <- importance / max(importance)



```



Describe the results of this SVM (5 pts)


# ROC analysis

Another measure for examining model performance is the ROC.

(a) Describe how an ROC works (2 pt)

(b) Report on the ROC results from the model (2 pt)

```{r,echo=TRUE,eval=FALSE,results='hide'}
library(ROCR)
p <- predict(svm.linear,testing_data)
pr <- prediction(as.numeric(p),as.numeric(testing_data$class))
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
```

# Data problems

Two of the factors that can make model fitting challenging are outliers and multicollinearity. Comment on whether those two issues are present in the data and whether they cause problems. Illustrate your comments with data/graphs as necessary.


# Model optimization

If our model is not very good, one issue could be that the model parameters are just not the best for describing the data. Here we find the best model parameters for a linear SVM. Use this code as a basis to report on how good this optimized model is (you can leave out the ROC analysis).

```{r}
# find best model parameter
tune_out = tune(svm, 
                class~., 
                data = equal_training_data, 
                kernel = "linear", 
                ranges = list(cost = c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune_out)
bestmod = tune_out$best.model
summary(bestmod)

# adjust the code to do the analysis for the best model

```


# Model comparison

Finally, we can compare different models. Apart from linear SVMs, we can use other SVMs with a radial basis function, which works by replacing the "linear" in the kernel with "radial", or "polynomial". Repeat the analysis for those two kernels and report on the results. Focus just on the model's predictions, as importances are not valid for nonlinear kernels. 

```{r}
# repeat the same classification with a radial SVM kernel
tune_out2 = tune(svm, 
                class~., 
                data = equal_training_data, 
                kernel = "radial", 
                ranges = list(cost = seq(0.01, 10, length.out = 20)))
summary(tune_out2)
bestmod2 = tune_out2$best.model
summary(bestmod2)
# And for a polynomial kernel
tune_out3 = tune(svm, 
                class~., 
                data = equal_training_data, 
                kernel = "polynomial", degree =2,
                ranges =list(cost = 10^seq(-2,1, by = 0.25)))
summary(tune_out3)
bestmod3 = tune_out3$best.model
summary(bestmod3)


# Which of these models is best? And why?
```



# Extending this to algorithms more generally

Note: To do this question, you probably want to check back to the Perusall reading belonging to this week.

(a) Say, you are working on creating a classifier to detect humans in satellite images. Can you use AIC in this case? If so, why? If not, why not? What other methods could you use to compare different versions of your classifier? What model comparison methods are good for what kinds of models? 

In these kinds of large computational projects, you always have to worry about overfitting. One of the readings for this week talks about the danger of overfitting. As we learnt in the reading for this week, overfitting is deceptively easy, even when you use cross-validation.

(b) How can overfitting still occur, even when you use cross-validation?

(c) How is the lockbox method (also known as train-test/validation) different from cross-validation, and why is it effective to prevent overfitting? 

(d) What other methods could you use to prevent overfitting? For each method you mention, briefly explain how it helps to prevent overfitting.

# Contributions

Indicate which person made what contribution to the assignment.